# -*- coding: utf-8 -*-
"""Notebook_GleichText.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l8GdLiRPA5QqbxkTN2eekXgKGPBDUiyx

# Step 1: Finetuning for gender bias detection
"""

!pip install -q transformers datasets evaluate scikit-learn matplotlib seaborn pandas

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    TrainerCallback,
)
import evaluate
import torch
import torch.nn.functional as F

from IPython.display import HTML, display, clear_output
import matplotlib.animation as animation

from google.colab import drive

#Model to use (single model)
MODEL_KEY = "german_bert"
MODEL_NAME = "bert-base-german-cased"

# Labels mapping (keep as in original)
ID2LABEL = {0: "non-biased", 1: "biased"}
LABEL2ID = {"non-biased": 0, "biased": 1}

# Training hyperparameters (copy from original)
LEARNING_RATE = 2e-5
BATCH_SIZE = 16
NUM_EPOCHS = 5
WEIGHT_DECAY = 0.01

# File path on Google Drive (change if needed)
CSV_PATH_IN_DRIVE = "/content/wortschatz_leipzig_extended.csv"

# Output directories
OUTPUT_DIR = f"./results_{MODEL_KEY}"
FINETUNED_DIR = f"./{MODEL_KEY}_finetuned"

# Mount Google Drive
drive.mount('/content/drive')

# Read CSV into pandas
df = pd.read_csv(CSV_PATH_IN_DRIVE)

# Convert to HuggingFace Dataset and rename "text" -> "sentence" if necessary
if "text" in df.columns and "sentence" not in df.columns:
    df = df.rename(columns={"text": "sentence"})
elif "sentence" not in df.columns:
    raise ValueError("CSV must contain a 'sentence' or 'text' column with the text to classify.")

dataset = Dataset.from_pandas(df)

# Split into train/validation/test (same proportions as original code)
train_test = dataset.train_test_split(test_size=0.2, seed=42)
test_valid = train_test['test'].train_test_split(test_size=0.5, seed=42)

dataset_dict = {
    'train': train_test['train'],
    'validation': test_valid['train'],
    'test': test_valid['test']
}

# Metrics
accuracy = evaluate.load("accuracy")
precision = evaluate.load("precision")
recall = evaluate.load("recall")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "precision": precision.compute(predictions=preds, references=labels, average="weighted")["precision"],
        "recall": recall.compute(predictions=preds, references=labels, average="weighted")["recall"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"],
    }

# Callback to record per-epoch eval metrics
class EpochMetricsRecorder(TrainerCallback):

    def __init__(self):
        self.epochs = []
        self.metrics_per_epoch = []  # list of dicts with keys accuracy, precision, recall, f1

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        # trainer.state.epoch contains current epoch number (float), metrics contains eval_* metrics
        if metrics is None:
            return
        # Extract epoch (convert to int if whole number)
        epoch = getattr(state, "epoch", None)
        if epoch is None:
            # fallback to length of recorded epochs + 1
            epoch = len(self.epochs) + 1
        # Metric keys may be like 'eval_accuracy'; strip "eval_" if present
        cleaned = {}
        for k, v in metrics.items():
            if k.startswith("eval_"):
                cleaned[k.replace("eval_", "")] = v
            elif k in ("accuracy", "precision", "recall", "f1"):
                cleaned[k] = v
        # Store only the 4 metrics
        entry = {m: cleaned.get(m, None) for m in ["accuracy", "precision", "recall", "f1"]}
        self.epochs.append(epoch)
        self.metrics_per_epoch.append(entry)

# Instantiate recorder
recorder = EpochMetricsRecorder()

# Train & Evaluate (single model)

def train_and_evaluate_single_model(model_name, dataset_dict, model_key):

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Tokenize dataset (padding/truncation to max length)
    def tokenize(batch):
        return tokenizer(batch["sentence"], padding="max_length", truncation=True)

    tokenized_datasets = {}
    for split, ds in dataset_dict.items():
        tokenized = ds.map(tokenize, batched=True)
        # rename column "label" -> "labels" if necessary (HuggingFace expects "labels")
        if "label" in tokenized.column_names and "labels" not in tokenized.column_names:
            tokenized = tokenized.rename_column("label", "labels")
        tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
        tokenized_datasets[split] = tokenized

    # Load model (classification head with 2 labels and label mapping)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        id2label=ID2LABEL,
        label2id=LABEL2ID
    )

    # Training arguments
    args = TrainingArguments(
        output_dir=f"{OUTPUT_DIR}",
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        num_train_epochs=NUM_EPOCHS,
        weight_decay=WEIGHT_DECAY,
        logging_dir="./logs",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        # reduce verbosity slightly
        logging_strategy="epoch",
        save_total_limit=3,
        seed=42
    )

    # Create Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[recorder]  # register callback to record epoch metrics
    )

    # Train model
    trainer.train()

    # Evaluate on test set (final)
    results = trainer.evaluate(tokenized_datasets["test"])
    print(f"\nüìä Final test results for {model_key}:")
    for k, v in results.items():
        print(f"  {k}: {v}")

    # Save model and tokenizer
    model.save_pretrained(FINETUNED_DIR)
    tokenizer.save_pretrained(FINETUNED_DIR)
    print(f"\nModel and tokenizer saved to: {FINETUNED_DIR}")

    return results, trainer, tokenizer, model, recorder

# Run training for single model
results, trainer_obj, tokenizer_obj, model_obj, recorder_obj = train_and_evaluate_single_model(MODEL_NAME, dataset_dict, MODEL_KEY)

def plot_metrics_over_epochs(recorder):

    epochs = recorder.epochs
    metrics_list = recorder.metrics_per_epoch

    if len(epochs) == 0:
        print("No epoch metrics recorded. Make sure eval_strategy='epoch' and recorder callback ran.")
        return


    metric_names = ["accuracy", "precision", "recall", "f1"]
    values_dict = {m: [entry[m] for entry in metrics_list] for m in metric_names}

    for m in metric_names:
        plt.figure(figsize=(7, 5))
        plt.plot(epochs, values_dict[m], marker="o", label=m)
        plt.title(f"{m.upper()} over epochs")
        plt.xlabel("Epoch")
        plt.ylabel(m.upper())
        plt.ylim(0, 1)
        plt.xticks(epochs)
        plt.grid(True, linestyle="--", alpha=0.6)
        plt.legend()
        plt.show()


plot_metrics_over_epochs(recorder_obj)


def plot_final_metrics(results_dict, recorder):

    if len(recorder.metrics_per_epoch) > 0:
        final_metrics = recorder.metrics_per_epoch[-1]
    else:
        final_metrics = {}
        for k, v in results_dict.items():
            if k.startswith("eval_"):
                final_metrics[k.replace("eval_", "")] = v
        final_metrics = {m: final_metrics.get(m, None) for m in ["accuracy", "precision", "recall", "f1"]}

    metrics = ["accuracy", "precision", "recall", "f1"]
    values = [final_metrics.get(m, 0.0) for m in metrics]

    plt.figure(figsize=(8,5))
    sns.barplot(x=metrics, y=values)
    plt.ylim(0, 1)
    plt.title("Final metrics (test / last eval)")
    plt.ylabel("Score")
    plt.xlabel("Metric")
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.3f}", ha='center')
    plt.show()

plot_final_metrics(results, recorder_obj)

# Example test sentences
test_sentence("Die Krankenschwester k√ºmmerte sich um die Patienten.", model_obj, tokenizer_obj)
test_sentence("Die Sch√ºler diskutierten ihre Forschungsergebnisse im Seminar.", model_obj, tokenizer_obj)
test_sentence("Die Studierende diskutierten ihre Forschungsergebnisse im Seminar.", model_obj, tokenizer_obj)

drive_dest = "/content/drive/MyDrive/german_bert_finetuned"
!cp -r {FINETUNED_DIR} {drive_dest}
print(f"Copied fine-tuned model to: {drive_dest}")

"""# Step 2: Paraphrasing"""

!pip install -q transformers datasets evaluate sacrebleu matplotlib seaborn pandas

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import torch
from datasets import Dataset
from transformers import (
    MBartForConditionalGeneration,
    MBart50TokenizerFast,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
import evaluate

from google.colab import drive

df = pd.read_csv("/content/wortschatz_leipzig_extended - intext_outtext.csv")
dataset = Dataset.from_pandas(df)

# Train/validation split
dataset = dataset.train_test_split(test_size=0.1, seed=42)

# Load model & tokenizer

model_name = "facebook/mbart-large-50-many-to-many-mmt"
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

# Set source and target language to German
tokenizer.src_lang = "de_DE"
tokenizer.tgt_lang = "de_DE"

#Preprocessing

def preprocess(examples):
    inputs = examples["input_text"]
    targets = examples["target_text"]

    model_inputs = tokenizer(inputs, max_length=128, padding="max_length", truncation=True)
    labels = tokenizer(targets, max_length=128, padding="max_length", truncation=True).input_ids


    labels = [[(id if id != tokenizer.pad_token_id else -100) for id in label] for label in labels]
    model_inputs["labels"] = labels
    return model_inputs

tokenized = dataset.map(preprocess, batched=True, remove_columns=dataset["train"].column_names)

# Define metrics

sacrebleu = evaluate.load("sacrebleu")

def compute_metrics(eval_pred):
    """
    Compute SacreBLEU score for generated sequences.
    """
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)


    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)


    result = sacrebleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])

    return {"bleu": result["score"]}

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# Training setup

training_args = Seq2SeqTrainingArguments(
    output_dir="./biased2neutral_de",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_strategy="steps",
    logging_steps=20,
    report_to="none",
    predict_with_generate=True,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="bleu"
)

# Data collator for seq2seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)


# Trainer

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

drive.mount('/content/drive')
drive_model_path = "/content/drive/MyDrive/biased2neutral_mbart_final"

model.save_pretrained(drive_model_path)
tokenizer.save_pretrained(drive_model_path)
print("Model and tokenizer saved to:", drive_model_path)

df_metrics = pd.DataFrame(trainer.state.log_history)
df_metrics.to_csv("training_metrics.csv", index=False)
print("Saved training metrics to training_metrics.csv")
print("Columns available:", df_metrics.columns)


df_metrics = pd.DataFrame(trainer.state.log_history)


df_epochs = df_metrics.dropna(subset=["epoch"])


df_epochs = df_epochs.fillna(method="ffill")


plt.figure(figsize=(8,5))
if "loss" in df_epochs.columns:
    plt.plot(df_epochs["epoch"], df_epochs["loss"], marker="o", label="Training Loss")
if "eval_loss" in df_epochs.columns:
    plt.plot(df_epochs["epoch"], df_epochs["eval_loss"], marker="s", label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training & Validation Loss per Epoch")
plt.legend()
plt.grid(True, linestyle="--", alpha=0.6)
plt.show()


if "eval_bleu" in df_epochs.columns:
    plt.figure(figsize=(8,5))
    plt.plot(df_epochs["epoch"], df_epochs["eval_bleu"], marker="d", color="green", label="BLEU")
    plt.xlabel("Epoch")
    plt.ylabel("BLEU Score")
    plt.title("BLEU Score per Epoch")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()

# Quick inference test

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

text = "Die Sch√ºler k√∂nnen selbst entscheiden, was sie tun m√∂chten."

# Tokenize and send input to the same device
inputs = tokenizer(text, return_tensors="pt").to(device)

# Generate
output_ids = model.generate(inputs["input_ids"], max_length=128, num_beams=4)

print("Original:", text)
print("Rewritten:", tokenizer.decode(output_ids[0], skip_special_tokens=True))

"""# Step 3: GleichText Implmentation"""

import torch
import torch.nn.functional as F
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    MBartForConditionalGeneration, MBart50TokenizerFast
)

# Before running this block, make sure you have set the correct paths to your trained models on Google Drive
# in the variables `bert_path` and `mbart_path` above.

bert_path = "/content/drive/MyDrive/german_bert_finetuned"  #<-- insert your BERT finetuned model path here
bert_tokenizer = AutoTokenizer.from_pretrained(bert_path)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_path)  #<-- insert your mBART finetuned model path here

mbart_path = "/content/drive/MyDrive/biased2neutral_mbart_final"
mbart_tokenizer = MBart50TokenizerFast.from_pretrained(mbart_path)
mbart_model = MBartForConditionalGeneration.from_pretrained(mbart_path)

mbart_tokenizer.src_lang = "de_DE"
mbart_tokenizer.tgt_lang = "de_DE"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model = bert_model.to(device)
mbart_model = mbart_model.to(device)

def check_and_rewrite(sentence: str):
    """
    1. Check sentence for gender bias with BERT classifier.
    2. If biased ‚Üí rewrite with mBART paraphraser.
    3. Return result as string.
    """

    # --- Step 1: Classification ---
    inputs = bert_tokenizer(sentence, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = bert_model(**inputs)
        probs = F.softmax(outputs.logits, dim=-1).cpu().numpy()[0]

    pred_label = int(probs.argmax())
    confidence = probs.max()

    # --- Step 2: Decision ---
    if pred_label == 0:  # non-biased
        return f"‚úÖ Im Text wurden keine Anzeichen f√ºr Gender Bias gefunden. (Sicherheit: {confidence:.2f})"
    else:
        # --- Step 3: Rewriting ---
        inputs_mbart = mbart_tokenizer(sentence, return_tensors="pt").to(device)
        with torch.no_grad():
            output_ids = mbart_model.generate(
                inputs_mbart["input_ids"],
                max_length=128,
                num_beams=4
            )
        rewritten = mbart_tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return (
            f"‚ö†Ô∏è Leider scheint der Text Gender Bias zu enthalten "
            f"(Sicherheit: {confidence:.2f}).\n"
            f"üëâ Vorschlag f√ºr eine umgeschriebene Version:\n\n{rewritten}"
        )

"""# Step 4: GleichText-Test"""

print(check_and_rewrite("Die Krankenschwester k√ºmmerte sich um die Patienten."))
print("------------------------------------------------------")
print(check_and_rewrite("Den Besuchern hat die neue Ausstellung sehr gut gefallen."))
print("------------------------------------------------------")
print(check_and_rewrite("Die Studierende diskutierten ihre Forschungsergebnisse im Seminar."))